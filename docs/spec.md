# MetaKey AI – Product & Engineering Specification

**Document-ID:** MK-SPEC-01  
**Revision:** v0.9 (draft, 2025-01-27)  
**Authors:** Max Rivest, Sota (Onetoken.ai)  
**License:** GPL-2.0 (same as Linux)  

## 1. Purpose & Scope

MetaKey AI is a cross-platform desktop assistant that lives in the clipboard/hot-key layer, runs user-provided "spells" (text→text), "whispers" (speech→text), "echoes" (text→speech) and "incantations" (multi-step wizards), and can be extended in any programming language. This document freezes MVP functionality and all public contracts for v1.0.

## 2. Glossary

- **Spell** - Text-in → Text-out script or server  
- **Whisper** - Audio-in → Text-out  
- **Echo** - Text-in → Audio-out  
- **Incant.** - Interactive multi-step spell  
- **Provider** - External LLM/TTS/STT service or local LLM server  
- **Recipe** - Shell/PS script that installs & starts a local provider  
- **System-Agent** - Native Rust binary handling hotkeys & clipboard  

## 3. Key Non-Functional Requirements

• Windows 7+, macOS 11+, Ubuntu 20.04+ – 64-bit  
• Zero admin rights; runs behind Zscaler / corporate proxies  
• Cold-start to hot-key response ≤ 150 ms  
• Clipboard history > 10 000 items encrypted AES-GCM  
• All internal comms no-network (stdin/stdout IPC)  
• Updatable without breaking signed binaries (Electron-Updater / Sparkle / AppImage update)  

## 4. High-Level Architecture (Tri-Layer)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│ Electron App "MetaKey AI"                                                  │
│ ┌─────────────┐  IPC(JSON)  ┌──────────┐  IPC(JSON)  ┌──────────────────┐ │
│ │ Rust Agent  │────────────▶│ Main Proc│────────────▶│ Renderer (UI)    │ │
│ │ hotkeys/CB  │◀────────────┤ Node.js  │◀────────────┤ React/Svelte/TW  │ │
│ └─────────────┘             └──────────┘            └──────────────────┘ │
│          ▲ spawn(subprocess,stdin/stdout JSON)                            │
│          │                                                               │
│          └───── Native Script Runtimes (Spells, Whispers, Echoes…)       │
└─────────────────────────────────────────────────────────────────────────────┘
```

## 5. Component Specifications

### 5.1 Rust System-Agent

- **target triple** - x86_64-pc-windows-msvc, x86_64-apple-darwin, x86_64-unknown-linux-gnu  
- **crates** - global-hotkey, arboard, serde_json  
- **events** - {event:"hotkey",shortcut:"ctrl-alt-w"} ; {event:"clipboard",content:"…"}  
- **build** - cargo build --release → packaged under app/resources/bin/<os>/system-agent  
- **lifetime** - always-on, respawned by Electron if exit ≠ 0  

### 5.2 Electron Main Process

- **Node 20 LTS**, TypeScript strict mode  
- **responsibility**  
  • decrypt/encrypt SQLite clipboard.db (better-sqlite3)  
  • spawn spells via resolveRuntime() → spawn(<runtime>,[script])  
  • manage shared cache (Redis 6 in-proc or sqlite mem)  
  • provider manager (API keys, local LLM URLs)  
  • auto-updater orchestration  

### 5.3 Renderer

- one always-on-top frameless window (widget) + one zen-mode modal  
- state is read-only via IPC "store:update" events  
- theme via CSS variables; plugins may add TW preset JSON  

## 6. IPC Contracts

### 6.1 Electron ⇆ System-Agent (JSON-line delimited UTF-8)

| request | response |
|---------|----------|
| `{cmd:"registerHotkey", id:"navLeft", shortcut:"Ctrl+Alt+Left"}` | `{ok:true}` |
| `{event:"hotkey", shortcut:"Ctrl+Alt+W"}` | (unsolicited) |

### 6.2 Electron ⇆ Spell Subprocess

**stdin** - one line JSON  
```json
{input:"…", model:"openai/gpt-4o", api_key:"XXXX"}
```

**stdout** - exactly one line JSON  
- **success** - `{output:"…", metadata:{t:123}}`  
- **error** - `{error:"msg", code:500, details:"stack"}`  

### 6.3 Renderer ⇆ Main (Electron IPC)

- `ipc.invoke("clipboard.read")` → `Promise<string>`  
- `ipc.invoke("spell.run",{id})` → `Promise<string|ErrorObj>`  
- `ipc.send("widget.show",{mode:"zen"})`  

## 7. Plugin Manifest (plugin.json v1)

```json
{
  "id": "spell.text-summarizer",
  "type": "spell" | "whisper" | "echo" | "incantation",
  "entry": "summarize.py",
  "shortcut": "ctrl-alt-s",
  "permissions": {
    "filesystem": "full" | "ro" | "none",
    "network": "full" | "none",
    "subprocess": true,
    "shell": true,
    "microphone": true   // whisper only
  },
  "providers_supported": ["openai","local-ollama"],
  "default_provider": "openai",
  "author": { 
    "name":"Alice", 
    "url":"https://git.example/alice" 
  }
}
```

## 8. Runtime Resolution

```javascript
resolveRuntime(lang):
  userPath = user_settings.runtime_paths[lang]
  if exists(userPath) return userPath
  managed = <appData>/runtimes/<lang>/bin/<exe>
  if exists(managed) return managed
  throw MissingRuntime(lang)
```

Managed runtimes installed via scripts in `recipes/`. Buttons in Settings run them via `child_process.exec`; logs to `setup.log`.

## 9. Local LLM Integration

Config (`user_settings.llm_backend`) as defined earlier.  
Recipe file pattern `recipes/<provider>_setup.(sh|ps1)`  
Managed mode: MainProcess starts provider server, health-checks on port.  

## 10. API-Key & Provider Passing

Main merges `allowed ∩ providers_supported → chosenProvider`  
IPC json includes model & api_key (if needed).  
api_key stored encrypted with OS keychain APIs.

## 11. Security & Trust

• First-run wizard explains power spells.  
• Install dialog lists manifest.permissions; user must click "I accept risk".  
• All downloaded content SHA-256 verified; binaries signed (ED25519).  
• Clipboard DB, api_key vault encrypted.  
• No telemetry by default; opt-in toggles send anonymised metrics to https://telemetry.metakey.ai (proxy-aware).

## 12. Logging

- **main.log** - general ops  
- **spells/<id>.log** - stdout/stderr of subprocess  
- **llm.log** - lifecycle of local providers  

All logs rotate at 10 MB, keep 5 files.

## 13. User-Facing Features (MVP)

- **F1** - open settings  
- **Ctrl+Alt+←/→** - cycle clipboard  
- **Ctrl+Alt+Q** - run default spell  
- **Ctrl+Alt+W** - toggle whisper record → transcribe→insert clipboard  
- **Ctrl+Alt+E** - echo current clipboard  
- **Widget hot-zone choices:** TL,TR,BL,BR,TM,ML,MR,BM  

## 14. Performance Budgets

- Agent event → UI update ≤ 40 ms  
- Spell cold start ( Python managed runtime ) ≤ 800 ms  
- Spell warm run ≤ 150 ms (plus external model time)  

## 15. Update Strategy

electron-builder Squirrel (Win), Sparkle (macOS), AppImageUpdate (Linux).  
Enterprises can place `freeze.json` next to executable to block updates.

## 16. Extending to New Language Runtime

• add `recipes/<lang>_setup.sh|ps1`  
• extend `resolveRuntime` map  
• add "Install <Lang> runtime" button with script path  

## 17. Marketplace Stub

Package index json served at https://market.metakey.ai/index.json  
Each entry contains plugin.zip SHA & manifest; downloaded to `plugins/`.  
Revenue share fields TBD in future spec.

## 18. Open Questions (to be closed before v1 freeze)

- [ ] Redis v SQLite-mem choice for shared cache  
- [ ] UI copy for "power-spell" warning  
- [ ] Exact AES key derivation path (OS-keychain vs PBKDF2)  

---

**End of specification – v0.9**

## Suggested Next Steps

1. Spin up a skeleton repo and commit this spec as `docs/spec.md`; open GitHub issues for every "Open Question".  
2. Prototype the Rust agent + Electron IPC loop on all three OS targets to validate hot-key latency.  
3. Draft first recipe (Ollama) and run E2E: Ctrl-Alt-Q → spell → local Llama 3 response in clipboard. 